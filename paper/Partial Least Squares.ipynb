{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Partial Least Squares (PLS)\n",
    "\n",
    "## 1. PCA\n",
    "Recall, in PCA, we are trying to achieve the objective\n",
    "$$\\max_w\\text{Var}\\left(\\mathbf{X}w\\right)$$\n",
    "$$\\text{subject to } ||w||_2^2=1;$$\n",
    "Intuitively, we are trying to find the directions in $\\mathbf{X}\\in \\mathbf{R}^{n \\times m}$ (n training examples, m features) that maximizes its variance. $\\mathbf{X}w = \\sum_i^{m}w_iX_i$.\n",
    "\n",
    "Suppose $\\mathbf{X}$ is mean centered. That is, $\\bar{\\mathbf{X}}_i=0$ $\\forall i=1, 2,..., m$. The solution is provided by SVD:\n",
    "$$\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T}$$\n",
    "\n",
    "where $\\mathbf{U}, \\mathbf{V}$ are orthonormal, and $\\mathbf{V}$ is the eigenvectors of sample covariance matrix $\\mathbf{S}=\\frac{1}{n-1} \\mathbf{X}^T \\mathbf{X}$; and $\\mathbf{U}$ is the eigenvectors of $\\mathbf{XX}^T$. $\\mathbf{\\Sigma}=\\text{diag}\\{\\sigma_1, \\sigma_2,...\\sigma_m\\} \\left(\\sigma_1 > \\sigma_2>...>\\sigma_m\\geq0\\right)$ contains standard deviations captured from new principle axes in descending order.\n",
    "\n",
    "SVD can be rearranged into\n",
    "$$\\mathbf{XV}=\\mathbf{U\\Sigma}=\\mathbf{T}$$\n",
    "\n",
    "The columns of $\\mathbf{U}$ (principal components) are the directions of maximum variance, while $\\mathbf{V}$ (loadings) is the rotation operator that transforms (unitary transformation) the column of $\\mathbf{X}$ to the columns of $\\mathbf{U\\Sigma}$ .\n",
    "$$\\mathbf{U\\Sigma}=\n",
    "\\begin{bmatrix}\n",
    "    \\vert & \\vdots & \\vert \\\\\n",
    "    \\sigma_1u_1   & \\vdots &\\sigma_mu_m   \\\\\n",
    "    \\vert & \\vdots &\\vert\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Often times, most of the variance in $\\mathbf{X}$ can be captured by the first few principal components.\n",
    "Suppose we take the first k â‰¤ m principal components. Then we can construct an\n",
    "approximation of $\\mathbf{X}$ via\n",
    "$$\\mathbf{X}_{:k}=\\mathbf{U}_{:k}\\mathbf{\\Sigma}_{:k}\\mathbf{V}_{:k}^{T}=\\sum_{i=1}^k \\sigma_i u_i v_i^T = \\text{sum of k rank 1 matrices}$$\n",
    "$$\\mathbf{T}_{:k}=\\mathbf{XV}_{:k}=\\mathbf{U}_{:k}\\mathbf{\\Sigma}_{:k}$$\n",
    "\n",
    "We then take matrix $\\mathbf{T}_{:k} \\in \\mathbf{R}^{n \\times k}$ to be our result of dimensionality reduction from $\\mathbf{X}$ and use the k columns of $\\mathbf{T}_{:k}$ as our new predictors. \n",
    "\n",
    "$$\\hat{y} = f\\left(\\mathbf{T}_{:k}\\right)$$\n",
    "\n",
    "where $f$ could be any machine learning model of our choice.\n",
    "\n",
    "## 2. Partial Least Squares (PLS)\n",
    "\n",
    "The problem with Principal Component Regression is that pricipal axes of largest variances in $\\mathbf{X}$ is not guaranteed to yield latent features that are good predictors of $y$. We can solve this by incorporating the information in $y$ into the dimensionality reduction process. One way to do this is to revise the objective of PCA into\n",
    "\n",
    "$$\\max_w\\text{Cov}\\left(\\mathbf{X}w, y\\right)$$\n",
    "$$\\text{subject to } ||w||_2^2=1;$$\n",
    "\n",
    "PLS component that has the highest possible covariance with $y$ under the constraint of being uncorrelated with all the previous components. This has to be solved iteratively, as there is no closed-form solution for all components.\n",
    "![alt text](https://i.imgur.com/yKf0PNs.png \"PLS1 Algorithm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 and 2 simply finds the unit vector that maximizes $\\text{argmax}_w\\text{Cov}\\left(\\mathbf{X}w, y\\right)=\\text{argmax}_ww^T\\mathbf{X}^Ty$, which is the dot product of vector $w$ and $\\mathbf{X}^Ty$. <br>\n",
    "Step 3 finds the $k$th principal component $t_k$ from <br>\n",
    "$$\\mathbf{T}=\\mathbf{XV}=\\mathbf{U}\\mathbf{\\Sigma}$$\n",
    "$$\\mathbf{T}=\n",
    "\\begin{bmatrix}\n",
    "    \\vert & \\vdots & \\vert & \\vdots & \\vert \\\\\n",
    "    t_1   & \\vdots & t_k & \\vdots & t_m   \\\\\n",
    "    \\vert & \\vdots & \\vert & \\vdots &\\vert\n",
    "\\end{bmatrix}\n",
    "=\\mathbf{U\\Sigma}=\n",
    "\\begin{bmatrix}\n",
    "    \\vert & \\vdots & \\vert & \\vdots  & \\vert \\\\\n",
    "    \\sigma_1u_1 & \\vdots & \\sigma_ku_k & \\vdots  &\\sigma_mu_m   \\\\\n",
    "    \\vert & \\vdots & \\vert & \\vdots  &\\vert\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Step 4 finds the $k$th column of the $\\mathbf{V}$ matrix by expoiting the fact that\n",
    "\n",
    "$$\\mathbf{X}=\\mathbf{T}\\mathbf{V}^{T}=\\sum_{i=1}^m t_i v_i^T=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T}=\\sum_{i=1}^m \\sigma_i u_i v_i^T$$\n",
    "Since\n",
    "$$\\mathbf{X}=\\mathbf{T}\\mathbf{V}^{T}$$\n",
    "$$\\mathbf{X}^T=\\mathbf{V}\\mathbf{T}^{T}$$\n",
    "$$\\mathbf{X}^T\\mathbf{T}=\\mathbf{V}\\mathbf{T}^{T}\\mathbf{T}$$\n",
    "$$\\mathbf{V}=\\mathbf{X}^T\\mathbf{T}\\left(\\mathbf{T}^{T}\\mathbf{T}\\right)^{-1}$$\n",
    "\n",
    "Step 5, 6, 7 simply updates $\\mathbf{X}$ and $y$ by subtracting derived components, or \"explained\" variance components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
